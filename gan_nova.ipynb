{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Deep Learning Brasil (www.deeplearningbrasil.com.br e https://github.com/deeplearningbrasil)\n",
    "Exemplo de aproximacao de funcao usando GAN - Generative Adversarial Networks - TensorFlow.\n",
    "\n",
    "Baseado no artigo original de Ian Goodfellow et. al.: https://arxiv.org/abs/1406.2661 e nos posts de Eric Jang: http://blog.evjang.com/2016/06/generative-adversarial-nets-in.html e Arthur Juliani https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39#.3gu2q43ah\n",
    "\n",
    "A tecnica de discriminante em minibatch foi obtida de Tim Salimans et. al.: https://arxiv.org/abs/1606.03498.\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistribuicaoDados(object):\n",
    "    def __init__(self):\n",
    "        self.media = 4\n",
    "        self.desvio_padrao = 0.5\n",
    "\n",
    "    def sample(self, N):\n",
    "        amostras = np.random.normal(self.media, self.desvio_padrao, N)\n",
    "        amostras.sort()\n",
    "        return amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistribuicaoDadosGerador(object):\n",
    "    def __init__(self, range):\n",
    "        self.range = range\n",
    "\n",
    "    def sample(self, N):\n",
    "        return np.linspace(-self.range, self.range, N) + \\\n",
    "            np.random.random(N) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(input, output_dim, scope=None, stddev=1.0):\n",
    "    norm = tf.random_normal_initializer(stddev=stddev)\n",
    "    const = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope(scope or 'linear'):\n",
    "        w = tf.get_variable('w', [input.get_shape()[1], output_dim], initializer=norm)\n",
    "        b = tf.get_variable('b', [output_dim], initializer=const)\n",
    "        return tf.matmul(input, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gerador(input, hidden_size):\n",
    "    passo1 = linear(input,hidden_size,'g0')\n",
    "    passo2 = tf.nn.softplus(passo1)\n",
    "    passo3 = linear(passo2, 1, 'g1')\n",
    "    return passo3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminador(input, h_dim, minibatch_layer=True):\n",
    "    passo1_h0 = linear(input,h_dim*2,'d0')\n",
    "    passo2_h0 = tf.tanh(passo1_h0)\n",
    "    passo1_h1 = linear(passo2_h0,h_dim*2,'d1')\n",
    "    passo2_h1 = tf.tanh(passo1_h1)\n",
    "\n",
    "    #Sem a camada de minibatch, o discriminador precisa de uma camada adicional \n",
    "    #para ter capacidade de separar as duas distribuicoes\n",
    "    if minibatch_layer:\n",
    "        passo3_h2 = minibatch(passo2_h1)\n",
    "    else:\n",
    "        passo3_h2 = tf.tanh(linear(passo2_h1, h_dim * 2, scope='d2'))\n",
    "\n",
    "    h3 = tf.sigmoid(linear(passo3_h2, 1, scope='d3'))\n",
    "    return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def otimizador(erro, var_list, taxa_aprendizado_inicial):\n",
    "    decaimento = 0.95\n",
    "    qtd_passos_decaimento = 150\n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        taxa_aprendizado_inicial,\n",
    "        batch,\n",
    "        qtd_passos_decaimento,\n",
    "        decaimento,\n",
    "        staircase=True\n",
    "    )\n",
    "    otimizador = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "        erro,\n",
    "        global_step=batch,\n",
    "        var_list=var_list\n",
    "    )\n",
    "    return otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DistribuicaoDados()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = DistribuicaoDadosGerador(range=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 1200\n",
    "batch_size = 12\n",
    "minibatch = False\n",
    "log_every = 10\n",
    "mlp_hidden_size = 4\n",
    "#anim_path = anim_path\n",
    "anim_frames = []\n",
    "learning_rate = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Cria a rede pre-treinada D_pre utilizando maximum likehood. A rede e \n",
    "# utilizada para repassar informacao do gradiente para a rede geradora \n",
    "# no primeiro passo.\n",
    "with tf.variable_scope('D_pre'):\n",
    "    pre_input = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "    pre_labels = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "    D_pre = discriminador(pre_input, mlp_hidden_size, minibatch)\n",
    "    pre_loss = tf.reduce_mean(tf.square(D_pre - pre_labels))\n",
    "    pre_opt = otimizador(pre_loss, None, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a rede geradora - esta obtem dados de entrada a partir de uma \n",
    "# distribuicao com ruidos, e repassa para uma rede MLP.\n",
    "with tf.variable_scope('Gen'):\n",
    "    z = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "    G = gerador(z, mlp_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# O discriminador tentara diferenciar as amostras reais (x) \n",
    "# das sinteticas (z)\n",
    "with tf.variable_scope('Disc') as scope:\n",
    "    x = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "    D1 = discriminador(x, mlp_hidden_size, minibatch)\n",
    "    scope.reuse_variables()\n",
    "    #cria uma segunda rede de discriminacao, pois no tensorflow nao seria \n",
    "    #possivel uma rede com duas entradas diferentes\n",
    "    D2 = discriminador(G, mlp_hidden_size, minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a funcao de erro, para a rede de discriminacao e de geracao\n",
    "loss_d = tf.reduce_mean(-tf.log(D1) - tf.log(1 - D2))\n",
    "loss_g = tf.reduce_mean(-tf.log(D2))\n",
    "\n",
    "d_pre_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D_pre')\n",
    "d_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Disc')\n",
    "g_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Gen')\n",
    "\n",
    "#define o otimizador para ambas as redes\n",
    "opt_d = otimizador(loss_d, d_params, learning_rate)\n",
    "opt_g = otimizador(loss_g, g_params, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss_d = []\n",
    "log_loss_g = []\n",
    "# discriminador pre-treinado\n",
    "num_pretrain_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for step in range(num_pretrain_steps):\n",
    "    d = (np.random.random(batch_size) - 0.5) * 10.0\n",
    "    labels = norm.pdf(d, loc=data.media, scale=data.desvio_padrao)\n",
    "    pretrain_loss, _ = session.run([pre_loss, pre_opt], {\n",
    "        pre_input: np.reshape(d, (batch_size, 1)),\n",
    "        pre_labels: np.reshape(labels, (batch_size, 1))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weightsD = session.run(d_pre_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copia os pesos da rede a partir da rede pre-treinada para a rede discriminativa D\n",
    "for i, v in enumerate(d_params):\n",
    "    session.run(v.assign(weightsD[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# atualiza o discriminador\n",
    "x = data.sample(batch_size)\n",
    "z = gen.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.2883316 ,  3.30082146,  3.5032832 ,  3.58183075,  3.63778012,\n",
       "        3.98024173,  4.06537193,  4.14472554,  4.2602804 ,  4.27052549,\n",
       "        4.41461179,  4.49261009])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.99375158, -6.53948529, -5.08931483, -3.63001797, -2.17751654,\n",
       "       -0.72708681,  0.73267037,  2.19171085,  3.63986103,  5.10056468,\n",
       "        6.5464166 ,  8.0073245 ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-14b58c6cc393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_d, _ = session.run([loss_d, opt_d], {\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m })\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "loss_d, _ = session.run([loss_d, opt_d], {\n",
    "    x: np.reshape(x, (batch_size, 1)),\n",
    "    z: np.reshape(z, (batch_size, 1))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
